{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1XSuTANpmsLSXY-LKysuYbpGDgeCdgjBV",
      "authorship_tag": "ABX9TyMfcu2J7ZV4OsJtShx1vqlQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niha002/AI-class/blob/main/Hw5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr4HxKvOwQyA"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd drive\n",
        "!pwd\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktU579vOzlpK",
        "outputId": "0c94935c-64dc-42fb-b45c-f6e74b75206c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset.\n",
        "dataset = keras.preprocessing.image_dataset_from_directory( '/content/drive/MyDrive/Colab Notebooks', batch_size=64, image_size=(200, 200))\n",
        " \n",
        "# For demonstration, iterate over the batches yielded by the dataset.\n",
        "for data, labels in dataset:\n",
        "   print(data.shape)  # (64, 200, 200, 3)\n",
        "   print(data.dtype)  # float32\n",
        "   print(labels.shape)  # (64,)\n",
        "   print(labels.dtype)  # int32\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSWTsKtZxFKA",
        "outputId": "c37291ee-964c-4fb9-b61c-d6178edb0de8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 files belonging to 1 classes.\n",
            "(1, 200, 200, 3)\n",
            "<dtype: 'float32'>\n",
            "(1,)\n",
            "<dtype: 'int32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = keras.preprocessing.text_dataset_from_directory('/content/drive/MyDrive/Colab Notebooks', batch_size=64)\n",
        " \n",
        "# For demonstration, iterate over the batches yielded by the dataset.\n",
        "for data, labels in dataset:\n",
        "   print(data.shape)  # (64,)\n",
        "   print(data.dtype)  # string\n",
        "   print(labels.shape)  # (64,)\n",
        "   print(labels.dtype)  # int32"
      ],
      "metadata": {
        "id": "bZN-zfCQz5po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turning strings into sequences of integer word indices"
      ],
      "metadata": {
        "id": "lraE9c_t0E2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        " \n",
        "# Example training data, of dtype `string`.\n",
        "training_data = np.array([[\"The 1st AI keras.\"], [\" for prediction.\"]])\n",
        " \n",
        "# Create a TextVectorization layer instance. It can be configured to either\n",
        "# return integer token indices, or a dense token representation (e.g. multi-hot\n",
        "# or TF-IDF). The text standardization and text splitting algorithms are fully\n",
        "# configurable.\n",
        "vectorizer = TextVectorization(output_mode=\"int\")\n",
        " \n",
        "# Calling `adapt` on an array or dataset makes the layer generate a vocabulary\n",
        "# index for the data, which can then be reused when seeing new data.\n",
        "vectorizer.adapt(training_data)\n",
        "\n",
        "# After calling adapt, the layer is able to encode any n-gram it has seen before\n",
        "# in the `adapt()` data. Unknown n-grams are encoded via an \"out-of-vocabulary\" token.\n",
        "integer_data = vectorizer(training_data)\n",
        "print(integer_data)\n"
      ],
      "metadata": {
        "id": "Z-yrlvevzwsg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a7d7730-5758-487b-98ca-05c785d778ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[2 7 6 4]\n",
            " [5 3 0 0]], shape=(2, 4), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turning strings into sequences of one-hot encoded bigrams"
      ],
      "metadata": {
        "id": "mACzgfdx0Ipo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        " \n",
        "# Example training data, of dtype `string`.\n",
        "training_data = np.array([[\"The 1st AI keras.\"], [\" for prediction.\"]])\n",
        "\n",
        "# Create a TextVectorization layer instance. It can be configured to either\n",
        "# return integer token indices, or a dense token representation (e.g. multi-hot\n",
        "# or TF-IDF). The text standardization and text splitting algorithms are fully\n",
        "# configurable.\n",
        "vectorizer = TextVectorization(output_mode=\"binary\", ngrams=2)\n",
        " \n",
        "# Calling `adapt` on an array or dataset makes the layer generate a vocabulary\n",
        "# index for the data, which can then be reused when seeing new data.\n",
        "vectorizer.adapt(training_data)\n",
        " \n",
        "# After calling adapt, the layer is able to encode any n-gram it has seen before\n",
        "# in the `adapt()` data. Unknown n-grams are encoded via an \"out-of-vocabulary\"\n",
        "# token.\n",
        "integer_data = vectorizer(training_data)\n",
        "print(integer_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWPynunz0N43",
        "outputId": "ef4a329a-6db0-4d7b-de0b-39faa1889329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1.]\n",
            " [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0.]], shape=(2, 11), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizing features"
      ],
      "metadata": {
        "id": "08eRzyki0RWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
        " \n",
        "# Example image data, with values in the [0, 500] range\n",
        "training_data = np.random.randint(0, 500, size=(100, 300, 200, 3)).astype(\"float32\")\n",
        " \n",
        "normalizer = Normalization(axis=-1)\n",
        "normalizer.adapt(training_data)\n",
        " \n",
        "normalized_data = normalizer(training_data)\n",
        "print(\"var: %.4f\" % np.var(normalized_data))\n",
        "print(\"mean: %.4f\" % np.mean(normalized_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6FGXnbA0Ut4",
        "outputId": "63f19d1b-1ce4-450f-bc72-0e3ae53934fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "var: 1.0011\n",
            "mean: 0.0005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rescaling & center-cropping images"
      ],
      "metadata": {
        "id": "tZS-nr350XiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import CenterCrop\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        " \n",
        "# Example image data, with values in the [0, 500] range\n",
        "training_data = np.random.randint(0, 500, size=(100, 300, 200, 3)).astype(\"float32\")\n",
        " \n",
        "cropper = CenterCrop(height=150, width=150)\n",
        "scaler = Rescaling(scale=1.0 / 255)\n",
        " \n",
        "output_data = scaler(cropper(training_data))\n",
        "print(\"shape:\", output_data.shape)\n",
        "print(\"min:\", np.min(output_data))\n",
        "print(\"max:\", np.max(output_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2apYai3e0aoB",
        "outputId": "76e98cfc-7c88-4854-b3df-33df3256b0cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (100, 150, 150, 3)\n",
            "min: 0.0\n",
            "max: 1.9568628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building Models/Networks with the Keras Functional API"
      ],
      "metadata": {
        "id": "DkNw_eRn0eGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dense = keras.layers.Dense(units=16)\n",
        "\n",
        "# Let's say we expect our inputs to be RGB images of arbitrary size\n",
        "inputs = keras.Input(shape=(200, 200, 3))\n",
        "\n",
        "from tensorflow.keras import layers\n",
        " \n",
        "# Center-crop images to 150x150\n",
        "x = CenterCrop(height=150, width=150)(inputs)\n",
        "# Rescale images to [0, 1]\n",
        "x = Rescaling(scale=1.0 / 255)(x)\n",
        " \n",
        "# Apply some convolution and pooling layers\n",
        "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=(3, 3))(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=(3, 3))(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
        " \n",
        "# Apply global average pooling to get flat feature vectors\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        " \n",
        "# Add a dense classifier on top\n",
        "num_classes = 10\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "data = np.random.randint(0, 256, size=(64, 200, 200, 3)).astype(\"float32\")\n",
        "processed_data = model(data)\n",
        "print(processed_data.shape)\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3aeb6D70hQz",
        "outputId": "8aea856a-cca5-437f-bc05-5451eadced5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 10)\n",
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_14 (InputLayer)       [(None, 200, 200, 3)]     0         \n",
            "                                                                 \n",
            " center_crop_3 (CenterCrop)  (None, 150, 150, 3)       0         \n",
            "                                                                 \n",
            " rescaling_4 (Rescaling)     (None, 150, 150, 3)       0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 148, 148, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 49, 49, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 47, 47, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 15, 15, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 13, 13, 32)        9248      \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 32)               0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 10)                330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,722\n",
            "Trainable params: 19,722\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Models/Networks with fit()\n"
      ],
      "metadata": {
        "id": "ZWaEVp_o0zUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select Optimizer and Loss Function \n",
        "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
        "              loss=keras.losses.CategoricalCrossentropy())\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n"
      ],
      "metadata": {
        "id": "Zv6Jlo2n03wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model/network\n",
        "model.fit(numpy_array_of_samples, numpy_array_of_labels,\n",
        "          batch_size=32, epochs=10)\n",
        "\n",
        "model.fit(dataset_of_samples_and_labels, epochs=10)"
      ],
      "metadata": {
        "id": "xsJ_cJFI2oL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example Networks and Training"
      ],
      "metadata": {
        "id": "mLzfsWd_2ueO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data as Numpy arrays\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        " \n",
        "# Build a simple model/network\n",
        "inputs = keras.Input(shape=(28, 28))\n",
        "x = layers.experimental.preprocessing.Rescaling(1.0 / 255)(inputs)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        " \n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
        " \n",
        "# Train the model for 1 epoch from Numpy data\n",
        "batch_size = 64\n",
        "print(\"Fit on NumPy data\")\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=1)\n",
        " \n",
        "# Train the model for 1 epoch using a dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
        "print(\"Fit on Dataset\")\n",
        "history = model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGhWRsRj2vfK",
        "outputId": "3209fda0-b37d-4970-a167-e75a2ae1a24c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 28, 28)]          0         \n",
            "                                                                 \n",
            " rescaling_2 (Rescaling)     (None, 28, 28)            0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               100480    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fit on NumPy data\n",
            "938/938 [==============================] - 10s 10ms/step - loss: 0.2619\n",
            "Fit on Dataset\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.1159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(history.history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tXkoKQP232t",
        "outputId": "621c31ab-eabd-483a-a3ae-1bf0c0073a81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': [0.11593519896268845]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monitoring metrics"
      ],
      "metadata": {
        "id": "HS0Kk5uA28cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
        ")\n",
        "history = model.fit(dataset, epochs=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0KlNm2127oE",
        "outputId": "420e8bb4-99bd-4470-c02b-5e1c828a3ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0811 - acc: 0.9753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Passing validation data to fit()\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "history = model.fit(dataset, epochs=1, validation_data=val_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az6OlX9Q3Eo2",
        "outputId": "56fe4451-a4af-4183-f12b-59981b641891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0557 - acc: 0.9833 - val_loss: 0.1064 - val_acc: 0.9694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using callbacks for checkpointing (and more)\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath='path/to/my/model_{epoch}',\n",
        "        save_freq='epoch')\n",
        "]\n",
        "model.fit(dataset, epochs=2, callbacks=callbacks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yIV45LQ3LYv",
        "outputId": "e8fd46c3-f63e-482c-8d81-cca9120110e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0403 - acc: 0.9879\n",
            "Epoch 2/2\n",
            "938/938 [==============================] - 5s 5ms/step - loss: 0.0320 - acc: 0.9902\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fda6b2e2610>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monitoring training progress with TensorBoard"
      ],
      "metadata": {
        "id": "ifU3VgoR3Roc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.TensorBoard(log_dir='./logs')\n",
        "]\n",
        "model.fit(dataset, epochs=2, callbacks=callbacks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WFIdBC-3S0d",
        "outputId": "5f8d4810-b240-402e-c26c-9dab40ed4e62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "938/938 [==============================] - 4s 4ms/step - loss: 0.0261 - acc: 0.9921\n",
            "Epoch 2/2\n",
            "938/938 [==============================] - 4s 4ms/step - loss: 0.0223 - acc: 0.9932\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fda6988b690>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After fit(): evaluating test performance & generating predictions on new data\n",
        "loss, acc = model.evaluate(val_dataset)  # returns loss and metrics\n",
        "print(\"loss: %.2f\" % loss)\n",
        "print(\"acc: %.2f\" % acc)\n",
        "\n",
        "predictions = model.predict(val_dataset)\n",
        "print(predictions.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5Ep4Bjo3xRd",
        "outputId": "d82da943-50a7-4e75-b6e5-3b34c379940c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 [==============================] - 1s 8ms/step - loss: 0.1124 - acc: 0.9721\n",
            "loss: 0.11\n",
            "acc: 0.97\n",
            "(10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using fit() with a Custom Training Step"
      ],
      "metadata": {
        "id": "KaXvOYau353M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomModel(keras.Model):\n",
        "  def train_step(self, data):\n",
        "    # Unpack the data. Its structure depends on your model and\n",
        "    # on what you pass to `fit()`.\n",
        "    x, y = data\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_pred = self(x, training=True)  # Forward pass\n",
        "      # Compute the loss value\n",
        "      # (the loss function is configured in `compile()`)\n",
        "      loss = self.compiled_loss(y, y_pred,\n",
        "                                regularization_losses=self.losses)\n",
        "    # Compute gradients\n",
        "    trainable_vars = self.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_vars)\n",
        "    # Update weights\n",
        "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "    # Update metrics (includes the metric that tracks the loss)\n",
        "    self.compiled_metrics.update_state(y, y_pred)\n",
        "    # Return a dict mapping metric names to current value\n",
        "    return {m.name: m.result() for m in self.metrics}\n",
        " \n",
        "# Construct and compile an instance of CustomModel\n",
        "inputs = keras.Input(shape=(32,))\n",
        "outputs = keras.layers.Dense(1)(inputs)\n",
        "model = CustomModel(inputs, outputs)\n",
        "model.compile(optimizer='adam', loss='mse', metrics=[...])\n",
        " \n",
        "# Just use `fit` as usual\n",
        "# model.fit(dataset, epochs=3, callbacks=...)"
      ],
      "metadata": {
        "id": "rEMAwX2I37_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debugging your Model"
      ],
      "metadata": {
        "id": "soTDRIfP4DjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='mse', run_eagerly=True)"
      ],
      "metadata": {
        "id": "XHcpmW3O4BOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Speeding up Training with Multiple GPUs"
      ],
      "metadata": {
        "id": "RFkXSqtP4Jll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a MirroredStrategy.\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        " \n",
        "# Open a strategy scope.\n",
        "with strategy.scope():\n",
        "  # Everything that creates variables should be under the strategy scope.\n",
        "  # In general this is only model construction & `compile()`.\n",
        "  model = Model(...)\n",
        "  model.compile(...)\n",
        " \n",
        "# Train the model on all available devices.\n",
        "train_dataset, val_dataset, test_dataset = get_dataset()\n",
        "model.fit(train_dataset, epochs=2, validation_data=val_dataset)\n",
        " \n",
        "# Test the model on all available devices.\n",
        "model.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "5k9rNTjD4Mut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing Preprocessing Synchronously "
      ],
      "metadata": {
        "id": "4lLgVCp-4TIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example training data, of dtype `string`.\n",
        "samples = np.array([[\"This is the 1st sample.\"], [\"And here's the 2nd sample.\"]])\n",
        "labels = [[0], [1]]\n",
        " \n",
        "# Prepare a TextVectorization layer.\n",
        "vectorizer = TextVectorization(output_mode=\"int\")\n",
        "vectorizer.adapt(samples)\n",
        " \n",
        "# Asynchronous preprocessing: the text vectorization is part of the tf.data pipeline.\n",
        "# First, create a dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((samples, labels)).batch(2)\n",
        "# Apply text vectorization to the samples\n",
        "dataset = dataset.map(lambda x, y: (vectorizer(x), y))\n",
        "# Prefetch with a buffer size of 2 batches\n",
        "dataset = dataset.prefetch(2)\n",
        " \n",
        "# Our model should expect sequences of integers as inputs\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(input_dim=10, output_dim=32)(inputs)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        " \n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", run_eagerly=True)\n",
        "model.fit(dataset)\n",
        "\n",
        "# Compare this to doing text vectorization as part of the model:\n",
        "# Our dataset will yield samples that are strings\n",
        "dataset = tf.data.Dataset.from_tensor_slices((samples, labels)).batch(2)\n",
        " \n",
        "# Our model should expect strings as inputs\n",
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = vectorizer(inputs)\n",
        "x = layers.Embedding(input_dim=10, output_dim=32)(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        " \n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", run_eagerly=True)\n",
        "model.fit(dataset)\n",
        "\n",
        "inputs = keras.Input(shape=(1,), dtype='string')\n",
        "x = vectorizer(inputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuVmKZdX4Ugm",
        "outputId": "ea82a133-2e4f-4fad-9e8b-9c310313fe7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 7 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fda6bdaa440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step - loss: 0.5117\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the Best Model Configuration"
      ],
      "metadata": {
        "id": "RBanFxrs4kss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner --upgrade"
      ],
      "metadata": {
        "id": "3l2UuXVeCVm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hp):\n",
        "    inputs = keras.Input(shape=(784,))\n",
        "    x = layers.Dense(\n",
        "        units=hp.Int('units', min_value=32, max_value=512, step=32,\n",
        "        activation='relu')(inputs)\n",
        "    outputs = layers.Dense(10, activation='softmax')(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            hp.Choice('learning_rate',\n",
        "                      values=[1e-2, 1e-3, 1e-4])),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "import kerastuner\n",
        " \n",
        "tuner = kerastuner.tuners.Hyperband(build_model, objective='val_loss', max_epochs=100,\n",
        "                                    max_trials=200, executions_per_trial=2, directory='my_dir')\n",
        "\n",
        "tuner.search(dataset, validation_data=val_dataset)\n",
        "models = tuner.get_best_models(num_models=2)\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "2LZ2RiHe4l46"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}